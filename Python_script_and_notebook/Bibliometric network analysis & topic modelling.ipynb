{"cells":[{"metadata":{"_uuid":"82e5f880-b70f-404c-b47d-6c217ee76317","_cell_guid":"debc761b-e9a2-43f6-bba4-3cf92e8ceb1a","trusted":true},"cell_type":"markdown","source":"# Bibliometric network analysis & topic modelling\n\nBibliometric data from academic databases can be used to find relationships between metadata (authors, titles, citations etc.) and discover dominant topics. In this kernel, we'll use the Metaknowledge package and an information science and bibliometrics dataset from Web of Science to perform network analysis and LDA topic modelling, along with visualizations. We'll try and answer the following questions:\n\n1. Which of the top authors are also top co-authors?\n2. What does the co-authorship network look like?\n3. What are the dominant topics that emerge from these academic papers?\n\nThe data and tutorials are available at https://github.com/mclevey/metaknowledge_article_supplement."},{"metadata":{"_uuid":"0f84937e-2920-431a-bb25-d24466c1fa5d","_cell_guid":"fa9208cf-236d-4caf-b143-c4221d607236","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\n!pip install metaknowledge # ensure Internet setting is \"On\" \nimport metaknowledge as mk\nimport networkx as nx\nimport community\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem.wordnet import WordNetLemmatizer\nimport spacy\nimport gensim\nimport gensim.corpora as corpora\nfrom gensim.utils import simple_preprocess\nfrom gensim.models import ldamodel\nfrom gensim.models import CoherenceModel \nimport re\nimport pyLDAvis\nimport pyLDAvis.gensim\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom pprint import pprint\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":0,"outputs":[]},{"metadata":{"_uuid":"ed634b7d-8606-4b61-bdb4-ed6927060d6a","_cell_guid":"c073c70a-c676-460b-9577-8d5b75d4c22b","trusted":true},"cell_type":"code","source":"# Importing the information science and bibliometrics dataset\nRC = mk.RecordCollection(\"../input/mk/raw_data/imetrics/\")\nlen(RC)","execution_count":0,"outputs":[]},{"metadata":{"_uuid":"70c9f00c-9e7f-495b-b8a6-7cf4b6ac907b","_cell_guid":"7cf09ebd-21af-4cef-8a8e-d740d2e9ac0d","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"RC","execution_count":0,"outputs":[]},{"metadata":{"_uuid":"49e8adae-9bc0-457a-a978-2c335c7f2f0c","_cell_guid":"a55978b4-4406-4ebe-8dda-c45e569295da","trusted":true},"cell_type":"markdown","source":"The data is currently stored as a RecordCollection object and must be converted into a dataframe if we want to see its contents. We can do this in two ways: with Pandas or with Metaknowledge's makeDict() function."},{"metadata":{"_uuid":"baf08e25-37aa-4db7-832c-237bc833a69c","_cell_guid":"0bec6179-380e-41c1-9357-8acf5335df7d","trusted":true},"cell_type":"code","source":"# Saving the dataset as a csv file\nRC.writeCSV(\"records.csv\")\n# Reading in the data as a Pandas dataframe\ndata = pd.read_csv(\"records.csv\")\ndata.head(3)","execution_count":0,"outputs":[]},{"metadata":{"_uuid":"ac5bb20e-c506-4cf4-b8f4-3e2c4ad79456","_cell_guid":"f4ff5c41-1d23-464f-ba5b-70f5971a569f","trusted":true},"cell_type":"code","source":"# Saving the data as a dataframe using mk's makeDict()\ndata2 = pd.DataFrame(RC.makeDict())\ndata2.head(3)","execution_count":0,"outputs":[]},{"metadata":{"_uuid":"e7002aaa-1d0f-450d-adb5-8c5a884319dd","_cell_guid":"6b9830ff-b8b3-4ba8-9334-4125526612f4","trusted":true},"cell_type":"markdown","source":"The makeDict() function gets rid of the id column and rearranges some columns, but the data is essentially the same. The two-letter variable names are tags used by Web of Science. See their description here - https://images.webofknowledge.com/images/help/WOK/hs_alldb_fieldtags.html\n\nMetaknowledge also provides the handy function glimpse() to view the top authors, journals and citations in the database."},{"metadata":{"_uuid":"32df2aca-322b-4c84-a6a0-cd718972266d","_cell_guid":"51d2c43b-ed89-4e54-9750-4a78784299e5","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Printing basic statistics about the data\nprint(RC.glimpse())","execution_count":0,"outputs":[]},{"metadata":{"_uuid":"581df899-ac93-4733-b834-3073e6b5ac2f","_cell_guid":"2f196aee-0cd9-440d-b26d-b136703daa2f","trusted":true},"cell_type":"markdown","source":"## Network Analysis\n\nNetwork analysis lets us see the relationships, distances and co-occurrences between the nodes in a network. Here's how we can conduct a network analysis of co-authors in this dataset."},{"metadata":{"_uuid":"f9126c5a-6c14-4855-bb21-63909ee6b3fd","_cell_guid":"72af3685-d65f-417d-875e-b7d25f921492","trusted":true},"cell_type":"code","source":"# Generating the co-author network \ncoauth_net = RC.networkCoAuthor()\ncoauth_net","execution_count":0,"outputs":[]},{"metadata":{"_uuid":"bcfaa517-2b8e-4232-b993-8f9dd5ef6786","_cell_guid":"6c49ab11-0aa6-43a8-b192-017b22d8b990","trusted":true},"cell_type":"code","source":"# Printing the network stats\nprint(mk.graphStats(coauth_net))","execution_count":0,"outputs":[]},{"metadata":{"_uuid":"7bfbfebb-b216-419d-b514-c9c180f733be","_cell_guid":"651220ab-b92d-4b72-8896-c3c67795b831","trusted":true},"cell_type":"markdown","source":"There are 10104 nodes (authors) in the network who are connected by 15507 edges. Of these authors, 1111 are isolates (unconnected to others).\n\nNext, we will drop self-loops and any authors with fewer than 2 edges (co-authors). For our analysis we will extract the \"giant component\", which is the largest subgraph of connected nodes in a network graph. The giant component typically contains a significant proportion of the nodes in the network. We'll use Python's networkx package for this and subsequent tasks."},{"metadata":{"_uuid":"c3e9dd7b-9d30-4599-868e-a038f385766a","_cell_guid":"1a499d29-5216-4586-803b-dc500eec6f64","trusted":true},"cell_type":"code","source":"mk.dropEdges(coauth_net, minWeight = 2, dropSelfLoops = True)\ngiant_coauth = max(nx.connected_component_subgraphs(coauth_net), key = len)\nprint(mk.graphStats(giant_coauth))","execution_count":0,"outputs":[]},{"metadata":{"_uuid":"869f5ad1-4e89-49b3-b81b-f452a9dbc1e1","_cell_guid":"e2a137cd-d456-4f15-86ba-4a092336a168","trusted":true},"cell_type":"markdown","source":"We are left with 265 authors, all of whom have at least two co-authors. We can see the graph density has gone up because of our filtering criteria.\n\nCentrality is a key concept in network analysis. The degree, closeness, betweenness and eigenvector centralities tell us which nodes (authors) are the most important. These are calculated from the number of links to other nodes, the length of the paths to other nodes, the number of times the node acts as a bridge along the shortest path between other nodes, and the relative influence of the node, respectively.\n\nLet's compute the centrality scores in our co-author graph."},{"metadata":{"_uuid":"ba6b88c5-e6d4-42f3-8706-43209d693ee1","_cell_guid":"1beb16bc-e2db-440c-b68f-714fb716a4e0","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Computing centrality scores\ndeg = nx.degree_centrality(giant_coauth)\nclo = nx.closeness_centrality(giant_coauth)\nbet = nx.betweenness_centrality(giant_coauth)\neig = nx.eigenvector_centrality(giant_coauth)\n\n# Saving the scores as a dataframe\ncent_df = pd.DataFrame.from_dict([deg, clo, bet, eig])\ncent_df = pd.DataFrame.transpose(cent_df)\ncent_df.columns = [\"degree\", \"closeness\", \"betweenness\", \"eigenvector\"]\n\n# Printing the top 10 co-authors by degree centrality score\ncent_df.sort_values(\"degree\", ascending = False)[:10]","execution_count":0,"outputs":[]},{"metadata":{"_uuid":"7f27eb99-e9e0-4de9-a935-c24a962c83d5","_cell_guid":"b45faf91-61e1-41c7-b040-4546eb2bbc0d","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Visualizing the top 10 co-authors by degree centrality score\nsns.set(font_scale=.75)\ncent_df_d10 = cent_df.sort_values('degree', ascending = False)[:10]\ncent_df_d10.index.name = \"author\"\ncent_df_d10.reset_index(inplace=True)\nprint()\nplt.figure(figsize=(10,7))\nax = sns.barplot(y = \"author\", x = \"degree\", data = cent_df_d10, palette = \"Set2\");\nax.set_alpha(0.8)\nax.set_title(\"Top 10 authors in co-author graph\", fontsize = 18)\nax.set_ylabel(\"Authors\", fontsize=14);\nax.set_xlabel(\"Degree centrality\", fontsize=14);\nax.tick_params(axis = 'both', which = 'major', labelsize = 14)","execution_count":0,"outputs":[]},{"metadata":{"_uuid":"d63d9c41-7e13-4477-a225-ec28b5743db4","_cell_guid":"17541f84-a46d-41c1-836f-df3edfeadaf9","trusted":true},"cell_type":"markdown","source":"The top 3 authors in the co-author network are the same as the top 3 authors in the original Record Collection. However, there are 5 authors in the original top 10 who are missing from the top 10 co-authors. \n\nLet's calculate the solo authorship and co-authorship rates for these 10 authors."},{"metadata":{"_uuid":"04bc309b-f905-4415-b1da-1894200b3f10","_cell_guid":"820c3f76-6f5c-42a2-a8e5-7a3618c85415","trusted":true},"cell_type":"code","source":"solo = []\nco = []\nfor i in cent_df_d10[\"author\"]:\n    # Calculate solo authorship rate\n    so = np.round((len(data[(data['AF'].str.contains(i)) & (data[\"num-Authors\"] == 1)])) / (len(data[data['AF'].str.contains(i)])), decimals = 2)\n    solo.append(so)\n    # Calculate co-authorship rate\n    co.append(1-so)\nprint(solo, co)","execution_count":0,"outputs":[]},{"metadata":{"_uuid":"a7377f53-97df-4244-9efe-40019afa6ce1","_cell_guid":"bf2f5c52-d25e-4b5c-a692-655546058ccf","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Create top 10 authors dataframe\nauthors = pd.DataFrame(zip(solo, co), columns = [\"solo\", \"coauthor\"])\nauthors[\"author\"] = cent_df_d10[\"author\"]\n# Rearrange columns\nauthors = authors[[\"author\", \"solo\", \"coauthor\"]]\nauthors","execution_count":0,"outputs":[]},{"metadata":{"_uuid":"767ca879-7ffd-4c64-8797-83de4078a660","_cell_guid":"5ce772ee-a5dc-4b93-99e3-ac27447f5b4a","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"sns.set(rc={'figure.figsize':(8,7)})\nfig = sns.swarmplot(x = \"solo\", y = \"coauthor\", hue = \"author\", data = authors, s = 12, palette = \"Paired\")\nplt.title(\"Solo vs. Co-authorship rate: Top 10 Authors\", fontsize = 16)\nplt.xlabel(\"Solo authorship rate\")\nplt.ylabel(\"Co-authorship rate\")\nplt.show()","execution_count":0,"outputs":[]},{"metadata":{"_uuid":"92bceeb6-4298-4fb3-80f0-b430e8e51af0","_cell_guid":"0cf30fd9-9813-44b3-a8d0-d36c9b25d7c4","trusted":true},"cell_type":"markdown","source":"Among the top 10 co-authors by degree centrality, Bornmann and Leydesdorff have the highest solo authorship rates while de Moya-Anegon and Wang have not authored any papers solo. \n\nNext, we will visualize the co-author network and then perform community detection using the Louvain method. The Louvain method maximizes a modularity score for each community or group of authors, which quantifies how \"good\" the communities are. (It does this by evaluating how much more densely connected the nodes within a community are, compared to how connected they would be in a random network.)\n\nNetwork visualizations can be difficult and confusing. There are several possible layouts, but we'll use the \"spring layout\" which results in a more aesthetic graph."},{"metadata":{"_uuid":"fac5dde4-fac0-4539-9188-c69540f392c2","_cell_guid":"b9818c5c-bf22-4f88-99bb-ed001a4354ed","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Visualizing the co-author network\nplt.figure(figsize = (10, 7))\nsize = [2000 * eig[node] for node in giant_coauth]\nnx.draw_spring(giant_coauth, node_size = size, with_labels = True, font_size = 5,\n               node_color = \"#FFFFFF\", edge_color = \"#D4D5CE\", alpha = .95)","execution_count":0,"outputs":[]},{"metadata":{"_uuid":"47914f38-2ffc-4d76-af41-154533afa3f1","_cell_guid":"3610a09c-9992-44a9-ad01-8f3c13fad384","trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"# Community detection\npartition = community.best_partition(giant_coauth) \nmodularity = community.modularity(partition, giant_coauth)\nprint(\"Modularity:\", modularity)","execution_count":0,"outputs":[]},{"metadata":{"_uuid":"2b28950b-9f52-4fc5-8862-dccaaaf294ea","_cell_guid":"68f183a6-d131-46bb-a72e-a29a4ac6807e","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Visualizing the communities\n# Generates a different graph each time\nplt.figure(figsize = (10, 7))\ncolors = [partition[n] for n in giant_coauth.nodes()]\nmy_colors = plt.cm.Set2 \nnx.draw(giant_coauth, node_color=colors, cmap = my_colors, edge_color = \"#D4D5CE\")","execution_count":0,"outputs":[]},{"metadata":{"_uuid":"ac2ce065-5f3f-4e70-aeb2-b519020e7bd4","_cell_guid":"0467d348-dbe5-48e3-a88b-d4fb2bac345c","trusted":true},"cell_type":"markdown","source":"The colourful visualization above reveals communities of co-authors in the network. Note that these are not fixed positions or shapes of the communities. The graph can look different each time.\n\nWe've analysed relationships between authors by computing and visualizing centralities in the co-author network. Next, we'll use NLTK and Gensim to preprocess and performing topic modelling on the academic journal data.\n\n## Topic Modelling\n\nThe Metaknowledge function forNLP() creates a Pandas-friendly dictionary where each row is a record from the RecordCollection, and the columns contain textual data (id, title, publication year, keywords and the abstract). Its results are *not* reproducible - the records appear to be shuffled each time.\n\nBefore we proceed, here's an overview of the ideas behind topic modelling with LDA:\n\n- Latent Dirichlet Allocation is a probabilistic model that generates topic-document and word-topic probability distributions. Topics are themes that occur in documents, and each word in a document belongs to some topic. \n- The word \"latent\" refers to the topics, which are hidden, while Dirichlet refers to the probability distribution over 'k' number of topics. Allocation refers to the process of assigning topics to documents and words to topics. \n- This allocation is an iterative process in which an initial topic distribution is assumed, and the weights for each word are updated till the model converges.\n- LDA tries to allocate words to a fewer number of topics in each document, and tries to assign high probabilities to only a few words within each topic. These are conflicting goals and the tradeoff results in groups of co-occuring words.\n- The model produces topics by inferring the conditional or posterior distribution over the latent variables, given the observed words and parameters."},{"metadata":{"_uuid":"bfebf4e5-7159-460e-923f-155492d25fb5","_cell_guid":"b80015a4-2e7f-4330-976b-74aa9b75858c","trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"# Transform the record collection into a format for use with natural language processing applications\ndata = RC.forNLP(\"topic_model.csv\", lower=True, removeNumbers=True, removeNonWords=True, removeWhitespace=True)\n\n# Convert the raw text into a list.\ndocs = data['abstract']\ndocs","execution_count":0,"outputs":[]},{"metadata":{"_uuid":"d6b5722f-d99f-48c8-97cc-1c6cb23cc18d","_cell_guid":"aa7b6664-9055-4ccb-8c36-b20e190a7d01","trusted":true},"cell_type":"markdown","source":"Since the data is in text form, we'll do some tokenizing and lemmatizing, remove stop words and create a term-document matrix to bring it into the right format for topic modelling."},{"metadata":{"_uuid":"81a53524-2c12-49b7-8b89-58bc5f213a12","_cell_guid":"80380236-a302-45b2-ba61-9d8ceb68a3ae","trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"# Defining a function to clean the text\ndef clean(docs):\n    # Insert function for preprocessing the text\n    def sent_to_words(sentences):\n        for sentence in sentences:\n            yield (simple_preprocess(str(sentence), deacc = True))\n    # Tokenize the text\n    tokens = sent_to_words(docs)\n    # Create stopwords set\n    stop = set(stopwords.words(\"english\"))\n    # Create lemmatizer\n    lmtzr = WordNetLemmatizer()\n    # Remove stopwords from text\n    tokens_stopped = [[word for word in post if word not in stop] for post in tokens]\n    # Lemmatize text\n    tokens_cleaned = [[lmtzr.lemmatize(word) for word in post] for post in tokens_stopped]\n    # Return cleaned text\n    return tokens_cleaned\n\n# Cleaning up the raw documents\ncleaned_docs = clean(docs)\ncleaned_docs","execution_count":0,"outputs":[]},{"metadata":{"_uuid":"127e9234-0a56-49a4-8e5d-660f0039994a","_cell_guid":"17da3e90-8c7c-4932-bfe4-faf91cf58bf3","trusted":true},"cell_type":"code","source":"# Creating a dictionary\nid2word = corpora.Dictionary(cleaned_docs)\nprint(id2word)","execution_count":0,"outputs":[]},{"metadata":{"_uuid":"36097652-4e2d-4be9-9eee-ee9167109825","_cell_guid":"c1e207e2-86d3-4fc1-b527-3494dfa980d8","trusted":true},"cell_type":"markdown","source":"There are 21943 unique words in the text. We'll filter out infrequent and overly frequent words from the dictionary, as this can improve the topic model."},{"metadata":{"_uuid":"433b123b-4ea1-4dbf-a6c2-27989d6116d2","_cell_guid":"4ca4ecf9-8035-43c4-bc6e-04a7d5e0e657","trusted":true},"cell_type":"code","source":"# Filtering infrequent and over frequent words\nid2word.filter_extremes(no_below=5, no_above=0.5)\n# Creating a document-term matrix\ncorpus = [id2word.doc2bow(doc) for doc in cleaned_docs]","execution_count":0,"outputs":[]},{"metadata":{"_uuid":"92e36e77-6dd0-47a0-ba87-5e4a93099be5","_cell_guid":"1a230941-acaf-4189-9867-e3af3e079df4","trusted":true},"cell_type":"code","source":"# Building an LDA model with 5 topics\nmodel = ldamodel.LdaModel(corpus = corpus, num_topics = 5, id2word = id2word, \n                              passes = 10, update_every = 1, chunksize = 1000, per_word_topics = True, random_state = 1)\n# Printing the topic-word distributions\npprint(model.print_topics())","execution_count":0,"outputs":[]},{"metadata":{"_uuid":"fa55f566-c005-4969-9512-b3d54bd87ee2","_cell_guid":"f252b242-3519-439d-a662-a23370d92852","trusted":true},"cell_type":"markdown","source":"The topics look well-defined and cohesive (remember we are using an Information Science and Bibliometrics dataset). Finally, we'll create an interactive visualization of the model that shows inter-topic distances and topic-word distributions."},{"metadata":{"_uuid":"49f6639b-ebc6-4cec-b56b-ac2d65129aa6","_cell_guid":"b1e7161e-eee7-43ff-9326-8c428cfde079","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"pyLDAvis.enable_notebook()\nvis = pyLDAvis.gensim.prepare(model, corpus, id2word, mds = \"tsne\")\nvis","execution_count":0,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":1}